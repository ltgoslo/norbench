{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc procedures during development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TSA conll data to DatasetDict\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "root_folder = \"data\"\n",
    "tsa_folder = os.path.join(root_folder,\"tsa_conll\")\n",
    "arrow_folder = os.path.join(root_folder,\"tsa_arrow_2\")\n",
    "\n",
    "def parse_conll(raw:str, sep=\"\\t\"):\n",
    "    \"\"\"Parses the norec-fine conll files with tab separator and sntence id\"\"\"\n",
    "    doc_parsed = [] # One dict per sentence. meta, tokens and tags\n",
    "    for sent in raw.strip().split(\"\\n\\n\"):\n",
    "        meta = \"\"\n",
    "        tokens, tags = [], []\n",
    "        for line in sent.split(\"\\n\"):\n",
    "            if line.startswith(\"#\") and \"=\" in line:\n",
    "                meta = line.split(\"=\")[-1]\n",
    "            else:\n",
    "                elems = line.strip().split(sep)\n",
    "                assert len(elems) == 2\n",
    "                tokens.append(elems[0])\n",
    "                tags.append(elems[1])\n",
    "        assert len(meta) > 0\n",
    "        doc_parsed.append({\"idx\": meta, \"tokens\":tokens, \"tsa_tags\":tags})\n",
    "\n",
    "    return doc_parsed\n",
    "\n",
    "\n",
    "splits = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"} # \"validation\" for HF naming convention\n",
    "d_sets = {}\n",
    "for split in splits:\n",
    "    path = os.path.join(tsa_folder, split+\".conll\")\n",
    "    with open(path) as rf:\n",
    "        conll_txt = rf.read()\n",
    "    print(len(conll_txt.split(\"\\n\\n\")))\n",
    "    sents = parse_conll(conll_txt)\n",
    "    # for sent in sents:\n",
    "        # sent[\"labels\"] = [label_mapping[tag] for tag in sent[\"tsa_tags\"]]\n",
    "    d_sets[splits[split]] = Dataset.from_pandas(pd.DataFrame(sents))\n",
    "\n",
    "DatasetDict(d_sets).save_to_disk(arrow_folder)\n",
    "    # sentences = parse(conll_txt)\n",
    "    # sentences[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"NorBERT_3_x-small\", \"NorBERT_3_small\", \"NorBERT_1\", \"NorBERT_2\", \"NB-BERT_base\", \"ScandiBERT\", \"mBERT\", \"XLM-R_base\", \"NorBERT_3_base\", \"XLM-R_large\", \"NB-BERT_large\", \"NorBERT_3_large\", \"Lenge-NorBERT_x-small\", \"Lenge-NorBERT_small\", \"Lenge-NorBERT_base\", \"NorT5_x-small\", \"NorT5_small\", \"NorT5_base\", \"NorT5_large\", \"Combined\", \"Oversampled\", \"NAK\", \"NCC\", \"mC4\", \"Wiki\", \"NBDigital\"]\n",
    "\n",
    "[\"NorBERT_3_x-small\", \"NorBERT_3_small\", \"NorBERT_1\", \"NorBERT_2\", \"NB-BERT_base\", \"ScandiBERT\", \"mBERT\", \"XLM-R_base\", \"NorBERT_3_base\", \"XLM-R_large\", \"NB-BERT_large\", \"NorBERT_3_large\",  \"Combined\", \"Oversampled\", \"NAK\", \"NCC\", \"mC4\", \"Wiki\", \"NBDigital\"]\n",
    "\n",
    "[\"Lenge-NorBERT_x-small\", \"Lenge-NorBERT_small\", \"Lenge-NorBERT_base\", \"NorT5_x-small\", \"NorT5_small\", \"NorT5_base\", \"NorT5_large\"]\n",
    "\n",
    "{\"NorBERT_3_x-small\": \"ltg/norbert3-xs\", \n",
    "\"NorBERT_3_small\": \"ltg/norbert3-small\", \n",
    "\"NorBERT_1\": \"ltg/norbert\" , \n",
    "\"NorBERT_2\":\"ltg/norbert2\",\n",
    "\"NB-BERT_base\": \"NbAiLab/nb-bert-base\",\n",
    "\"ScandiBERT\": \"vesteinn/ScandiBERT\", \n",
    "\"mBERT\": \"bert-base-multilingual-cased\", \n",
    "\"XLM-R_base\": \"xlm-roberta-base\",  \n",
    "\"NorBERT_3_base\":\"ltg/norbert3-base\",\n",
    "\"XLM-R_large\": \"xlm-roberta-large\",\n",
    "\"NB-BERT_large\": \"NbAiLab/nb-bert-large\", \n",
    "\"NorBERT_3_large\": \"ltg/norbert3-large\", Â \n",
    "\"Combined\": \"ltg/norbert3-base\", \n",
    "\"Oversampled\":\"ltg/norbert3-oversampled-base\", \n",
    "\"NAK\": \"ltg/norbert3-nak-base\", \n",
    "\"NCC\":\"ltg/norbert3-ncc-base\",\n",
    "\"mC4\":\"ltg/norbert3-c4-base\",\n",
    "\"Wiki\": \"ltg/norbert3-wiki-base\",\n",
    "\"NBDigital\": \"ltg/norbert3-nb-base\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# If you want a list of models with where to get them from\n",
    "models = {\n",
    "\"NorBERT_3_x-small\": \"ltg/norbert3-xs\", \n",
    "\"NorBERT_3_small\": \"ltg/norbert3-small\", \n",
    "\"NorBERT_1\": \"ltg/norbert\" , \n",
    "\"NorBERT_2\":\"ltg/norbert2\",\n",
    "\"NB-BERT_base\": \"NbAiLab/nb-bert-base\",\n",
    "\"ScandiBERT\": \"vesteinn/ScandiBERT\", \n",
    "\"mBERT\": \"bert-base-multilingual-cased\", \n",
    "\"XLM-R_base\": \"xlm-roberta-base\",  \n",
    "\"NorBERT_3_base\":\"ltg/norbert3-base\",\n",
    "\"XLM-R_large\": \"xlm-roberta-large\",\n",
    "\"NB-BERT_large\": \"NbAiLab/nb-bert-large\", \n",
    "\"NorBERT_3_large\": \"ltg/norbert3-large\",\n",
    "}\n",
    "\n",
    "with open(\"configs/models_name_addr1.json\", \"w\", encoding=\"utf8\") as wf:\n",
    "    json.dump(models, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under here, analysis of the json log files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': '04201157_tsa_NorBERT_3_base', 'epoch': 2, 'eval_f1': 0.5235955056179775}\n",
      "{'idx': '04201157_tsa_NB-BERT_base', 'epoch': 6, 'eval_f1': 0.5252873563218391}\n",
      "{'idx': '04201157_tsa_XLM-R_base', 'epoch': 8, 'eval_f1': 0.4937192790824686}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "# Load json files with details on each epoch for each experiment. Get the best epoch\n",
    "jsons = [(p.stem[:-4] ,json.loads(p.read_text()) )for p in Path(\"logs/jsons\").iterdir()]\n",
    "best_epochs = []\n",
    "for r in jsons:\n",
    "    idx = r[0]\n",
    "    epoch_eval = [ee for ee in r[1] if \"eval_f1\" in ee]\n",
    "    epoch_eval = sorted(epoch_eval, key = lambda l: l[\"eval_f1\"], reverse=True)\n",
    "    best_epochs.append({\"idx\": idx, \n",
    "                    \"epoch\": int(epoch_eval[0][\"epoch\"]),\n",
    "                    \"eval_f1\": epoch_eval[0][\"eval_f1\"] })\n",
    "    print(best_epochs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>second_best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>XLM-R_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.493719</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XLM-R_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.491877</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NorBERT_3_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.523596</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NorBERT_3_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.523167</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NB-BERT_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.525287</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NB-BERT_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.519016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model task  epoch   eval_f1  best_epoch  second_best\n",
       "20      XLM-R_base  tsa    8.0  0.493719        True        False\n",
       "21      XLM-R_base  tsa    7.0  0.491877       False         True\n",
       "0   NorBERT_3_base  tsa    2.0  0.523596        True        False\n",
       "1   NorBERT_3_base  tsa    3.0  0.523167       False         True\n",
       "10    NB-BERT_base  tsa    6.0  0.525287        True        False\n",
       "11    NB-BERT_base  tsa    7.0  0.519016       False         True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "records = [] # List of dicts that have the model name injected  \n",
    "for log_path in Path(\"logs/jsons\").iterdir():\n",
    "    p_stem = log_path.stem\n",
    "    stem_segments = p_stem.split(\"_\")\n",
    "    task = stem_segments[1]\n",
    "    ts = stem_segments[0]\n",
    "    m_name = p_stem[13:-4]\n",
    "    log = json.loads(log_path.read_text())\n",
    "\n",
    "    epoch_eval = [ee for ee in log if \"eval_f1\" in ee]\n",
    "    epoch_eval = sorted(epoch_eval, key = lambda l: l[\"eval_f1\"], reverse=True)\n",
    "    for i, epoch_log in enumerate(epoch_eval):\n",
    "        epoch_log.update({\"timestamp\":ts,\"model\":m_name, \"task\":task,\"best_epoch\": i==0, \"second_best\": i==1})\n",
    "        records.append(epoch_log)\n",
    "\n",
    "df_all = pd.DataFrame.from_records(records)\n",
    "# Function to filter the df_all according to True in \"best_epoch\" or \"second_best\"\n",
    "\n",
    "\n",
    "df = df_all[(df_all[\"best_epoch\"]== True) | (df_all[\"second_best\"]==True) ].sort_values([\"task\", \"eval_f1\"], ascending=False)\n",
    "df[[\"model\", \"task\", \"epoch\", \"eval_f1\", \"best_epoch\", \"second_best\"]].to_clipboard()\n",
    "# df.to_csv(\"output/dev_evals.csv\", index=False) # Write this for reporting and analysis\n",
    "\n",
    "\n",
    "df[[\"model\", \"task\", \"epoch\", \"eval_f1\", \"best_epoch\", \"second_best\"]].sort_values([\"model\", \"eval_f1\"], ascending=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below script is redundant after I moved saving best epoch to the config json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'NorBERT_3_base',\n",
       "  'task': 'tsa',\n",
       "  'epochs': 2,\n",
       "  'eval_f1': 0.5235955056179775,\n",
       "  'tested': False},\n",
       " {'model': 'NB-BERT_base',\n",
       "  'task': 'tsa',\n",
       "  'epochs': 6,\n",
       "  'eval_f1': 0.5252873563218391,\n",
       "  'tested': False},\n",
       " {'model': 'XLM-R_base',\n",
       "  'task': 'tsa',\n",
       "  'epochs': 8,\n",
       "  'eval_f1': 0.4937192790824686,\n",
       "  'tested': False}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create setup dict structure for the final testing with their best epochs from dev testing\n",
    "# With this you can train new models with the best epochs from dev testing for a number of times to get mean and std.\n",
    "# You can write a script based on seq_label.py to load the information in the json and train each model with different seeds, and test on the test split\n",
    "\n",
    "test_setup = []\n",
    "for rec in records:\n",
    "    if rec[\"best_epoch\"]:\n",
    "        test_setup.append({\"model\": rec[\"model\"], \"task\":rec[\"task\"], \"epochs\": int(rec[\"epoch\"]), \"eval_f1\":rec[\"eval_f1\"] , \"tested\":False})  \n",
    "Path(\"configs/evals\").mkdir(exist_ok=True, parents=True)\n",
    "# Path(\"configs/evals/test_setup.json\").write_text(json.dumps(test_setup))\n",
    "test_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just another view of the dataframe\n",
    "cols = list(df_all.columns)\n",
    "cols = [c for c in cols if not any([d in c for d in [\"second\", \"step\", \"runtime\"]])]\n",
    "df_all[cols][(df_all.task == \"tsa\") & (df_all.model == \"NorBERT_3_base\")].sort_values(\"eval_f1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f766e18cf02043d406c2f113693a415f1494f09983f05ef5cfd3ee3ed0acbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
